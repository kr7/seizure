{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kr7/seizure/blob/main/SeizureDetection_ROCKET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqSAzuUhxf86"
      },
      "source": [
        "**Activity Recognition Based on Accelerometer Data with Enhanced ROCKET Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pms23tSic5KY"
      },
      "outputs": [],
      "source": [
        "# dataset may be set to \"Epilepsy\" or \"OpenSeizure\"\n",
        "# - \"Epilepsy\" denotes the dataset that is publicly available from\n",
        "#   https://timeseriesclassification.com/aeon-toolkit/Epilepsy.zip\n",
        "# - If you want to use the \"OpenSeizure\" data, you should first execute\n",
        "#   the script to preprocess data. This notebook will ask you to upload\n",
        "#   the preprocessed data.\n",
        "dataset = \"Epilepsy\"\n",
        "\n",
        "# Set it to True in order to generate univariate convolutional kernels and\n",
        "# to assign them to channels randomly\n",
        "UNIVARIATE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1ziwkrzZXJN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from numpy import random as rnd\n",
        "from scipy.io import arff\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import RidgeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2kwidw_PUQI"
      },
      "outputs": [],
      "source": [
        "if dataset == \"Epilepsy\":\n",
        "  os.system(\"wget https://timeseriesclassification.com/aeon-toolkit/Epilepsy.zip\")\n",
        "  os.system(\"unzip -q Epilepsy.zip\")\n",
        "\n",
        "  # In order to perform 10-fold cross-validation, we\n",
        "  # will merge the provided train and test splits and\n",
        "  # we will split the data durign the cross-validation\n",
        "\n",
        "  NUM_CLASSES = 4\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  for filename in ['Epilepsy_TRAIN.arff', 'Epilepsy_TEST.arff' ]:\n",
        "    raw_data = arff.loadarff(filename)\n",
        "\n",
        "    nextInstance = True\n",
        "    i = 0\n",
        "\n",
        "    while nextInstance:\n",
        "      try:\n",
        "        dim1 = list(raw_data[0][i][0][0])\n",
        "        dim2 = list(raw_data[0][i][0][1])\n",
        "        dim3 = list(raw_data[0][i][0][2])\n",
        "        label = raw_data[0][i][1]\n",
        "\n",
        "        X.append([dim1,dim2,dim3])\n",
        "        if label == b'EPILEPSY':\n",
        "          label = 0\n",
        "        elif label == b'WALKING':\n",
        "          label = 1\n",
        "        elif label == b'RUNNING':\n",
        "          label = 2\n",
        "        elif label == b'SAWING':\n",
        "          label = 3\n",
        "        else:\n",
        "          print(f'Unexpected label: {label}')\n",
        "        y.append(label)\n",
        "\n",
        "        i = i+1\n",
        "      except:\n",
        "        nextInstance = False\n",
        "\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "elif dataset == \"OpenSeizure\":\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  # Read (preprocessed) OpenSeizure data\n",
        "\n",
        "  NUM_CLASSES = 2\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  with open('OpenSeizure.txt', 'r') as f:\n",
        "    nextInstance = True\n",
        "    while nextInstance:\n",
        "      try:\n",
        "        dim1 = [ float(v) for v in f.readline().split(\",\")]\n",
        "        dim2 = [ float(v) for v in f.readline().split(\",\")]\n",
        "        dim3 = [ float(v) for v in f.readline().split(\",\")]\n",
        "        label = int(f.readline())\n",
        "\n",
        "        X.append([dim1,dim2,dim3])\n",
        "        y.append(label)\n",
        "      except:\n",
        "        nextInstance = False\n",
        "\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "else:\n",
        "  raise Exception(f\"Unknown dataset: {dataset}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CHANNELS = 3"
      ],
      "metadata": {
        "id": "GPP2R_CMPZJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ18Ds5YjR1d"
      },
      "outputs": [],
      "source": [
        "%load_ext cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRw7dOrYkF6C"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "\n",
        "cimport cython\n",
        "from libc.stdlib cimport malloc, free\n",
        "\n",
        "\n",
        "def dtw(ts1p, ts2p):\n",
        "    cdef int LEN_TS1\n",
        "    cdef int LEN_TS2\n",
        "    cdef int i\n",
        "    cdef int j\n",
        "    cdef float * ts1\n",
        "    cdef float * ts2\n",
        "    cdef float * dtw_matrix\n",
        "    cdef float d\n",
        "\n",
        "    LEN_TS1 = len(ts1p)\n",
        "    LEN_TS2 = len(ts2p)\n",
        "\n",
        "    ts1 = <float *> malloc(LEN_TS1*cython.sizeof(float))\n",
        "    ts2 = <float *> malloc(LEN_TS2*cython.sizeof(float))\n",
        "    dtw_matrix = <float *> malloc(LEN_TS1*LEN_TS2*cython.sizeof(float))\n",
        "      # this is a flattened DTW matrix dtw_matrix[i,j] -> dtw_matrix[i*LEN_TS2 + j]\n",
        "    if ts1 is NULL or ts2 is NULL:\n",
        "      raise MemoryError()\n",
        "    for i in xrange(LEN_TS1):\n",
        "      ts1[i] = ts1p[i]\n",
        "    for i in xrange(LEN_TS2):\n",
        "      ts2[i] = ts2p[i]\n",
        "\n",
        "    dtw_matrix[0] = abs(ts1[0]-ts2[0])\n",
        "\n",
        "    for i in range(1, LEN_TS1):\n",
        "      dtw_matrix[i*LEN_TS2] = dtw_matrix[(i-1)*LEN_TS2]+abs(ts1[i]-ts2[0])\n",
        "\n",
        "    for j in range(1, LEN_TS2):\n",
        "      dtw_matrix[j] = dtw_matrix[j-1]+abs(ts1[0]-ts2[j])\n",
        "\n",
        "    for i in range(1, LEN_TS1):\n",
        "      for j in range(1, LEN_TS2):\n",
        "        dtw_matrix[i*LEN_TS2+j] = min(dtw_matrix[(i-1)*LEN_TS2+j-1],\n",
        "                                      dtw_matrix[(i-1)*LEN_TS2+j],\n",
        "                                      dtw_matrix[i*LEN_TS2+j-1]) + abs(ts1[i]-ts2[j])\n",
        "\n",
        "    d=dtw_matrix[ LEN_TS1*LEN_TS2-1 ]\n",
        "\n",
        "    free(ts1)\n",
        "    free(ts2)\n",
        "    free(dtw_matrix)\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Je9FlCbn5S"
      },
      "outputs": [],
      "source": [
        "def log2(x):\n",
        "  return np.log(x)/np.log(2)\n",
        "\n",
        "\n",
        "def get_random_convolution_params(seed, l_input):\n",
        "  rnd.seed(seed)\n",
        "  length = rnd.randint(0,2)*2+7\n",
        "  weights = rnd.normal(0,1, (NUM_CHANNELS, length) )\n",
        "  bias = float(rnd.uniform(-1, 1))\n",
        "  max_exp = np.floor(log2((l_input-1)/(length-1)))-1\n",
        "  dilation = int( 2**(rnd.uniform(0,max_exp)) )\n",
        "  padding = rnd.randint(0,1)\n",
        "  return (length, weights, bias, dilation, padding)\n",
        "\n",
        "\n",
        "def get_random_convolution_params_univariate(seed, l_input):\n",
        "  rnd.seed(seed)\n",
        "  length = rnd.randint(0,2)*2+7\n",
        "  weights = rnd.normal(0,1, (length) )\n",
        "  bias = float(rnd.uniform(-1, 1))\n",
        "  max_exp = np.floor(log2((l_input-1)/(length-1)))-1\n",
        "  dilation = int( 2**(rnd.uniform(0,max_exp)) )\n",
        "  padding = rnd.randint(0,1)\n",
        "  channel = rnd.randint(0,NUM_CHANNELS-1)\n",
        "  return (length, weights, bias, dilation, padding, channel)\n",
        "\n",
        "def apply_convolution( time_series, convolution ):\n",
        "  length, weights, bias, dilation, padding = convolution\n",
        "  window_size = length*dilation\n",
        "  time_series = list(time_series)\n",
        "\n",
        "  if padding == 1:\n",
        "    zeros = [0]*(dilation*(length-1)/2)\n",
        "    for i in range(NUM_CHANNELS):\n",
        "      time_series[i] = zeros + time_series[i] + zeros\n",
        "\n",
        "  segments = np.array(\n",
        "      [ [ time_series[j][i:i+window_size:dilation] for j in range(NUM_CHANNELS) ]\n",
        "        for i in range(0, int(len(time_series[0])-window_size)) ])\n",
        "\n",
        "  conv = np.array([np.sum(s*weights) + bias for s in segments])\n",
        "  dtw_conv = np.array( [ np.sum([dtw(s[c], weights[c]) for c in range(len(s))]) for s in segments] )\n",
        "\n",
        "  return conv, dtw_conv\n",
        "\n",
        "\n",
        "def apply_convolution_univariate( time_series, convolution ):\n",
        "  length, weights, bias, dilation, padding, channel = convolution\n",
        "  window_size = length*dilation\n",
        "  time_series = list(time_series)\n",
        "\n",
        "  if padding == 1:\n",
        "    zeros = [0]*(dilation*(length-1)/2)\n",
        "    for i in range(NUM_CHANNELS):\n",
        "      time_series[i] = zeros + time_series[i] + zeros\n",
        "\n",
        "  segments = np.array(\n",
        "      [ time_series[channel][i:i+window_size:dilation]\n",
        "        for i in range(0, int(len(time_series[0])-window_size)) ])\n",
        "\n",
        "  conv = np.array([np.sum(s*weights) + bias for s in segments])\n",
        "  dtw_conv = np.array( [ dtw(s, weights) for s in segments] )\n",
        "\n",
        "  return conv, dtw_conv\n",
        "\n",
        "\n",
        "def ppv(series):\n",
        "  return float(np.sum(series > 0) / np.size(series))\n",
        "\n",
        "\n",
        "def get_rocket_features(time_series_dataset, convolutional_filters,\n",
        "                        f_convolution = apply_convolution):\n",
        "  dataset_features_max = [] # features with conventional convolution\n",
        "  dataset_features_ppv = [] # features with conventional convolution\n",
        "  dataset_features_dtw_min = [] # features with dynamic convolution\n",
        "  i = 0\n",
        "  for ts in time_series_dataset:\n",
        "    if i%1==0:\n",
        "      print(f\"{i:4}/{len(time_series_dataset)}\")\n",
        "    i = i+1\n",
        "\n",
        "    ts_features_max = []\n",
        "    ts_features_ppv = []\n",
        "    ts_features_dtw_min = []\n",
        "    for c in convolutional_filters:\n",
        "      convolved_ts, dtw_convolved_ts = f_convolution(ts, c)\n",
        "      ts_features_max.append( float(max(convolved_ts)) )\n",
        "      ts_features_ppv.append( ppv(convolved_ts) )\n",
        "      ts_features_dtw_min.append( float(min(dtw_convolved_ts)) )\n",
        "       # PPV does not make sense for DTW!\n",
        "    dataset_features_max.append( ts_features_max )\n",
        "    dataset_features_ppv.append( ts_features_ppv )\n",
        "    dataset_features_dtw_min.append( ts_features_dtw_min )\n",
        "  return np.array(dataset_features_max), np.array(dataset_features_ppv), \\\n",
        "    np.array(dataset_features_dtw_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEwvPNM5IY0s"
      },
      "outputs": [],
      "source": [
        "def evaluate(pred, y_test):\n",
        "  err = np.mean(pred != y_test)\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "  for i in range(len(pred)):\n",
        "    if pred[i] == 0 and y_test[i] == 0:\n",
        "      tp = tp+1\n",
        "    elif pred[i] == 0 and y_test[i] != 0:\n",
        "      fp = fp+1\n",
        "    elif pred[i] != 0 and y_test[i] == 0:\n",
        "      fn = fn+1\n",
        "    elif pred[i] != 0 and y_test[i] != 0:\n",
        "      tn = tn+1\n",
        "    else:\n",
        "      raise Exception(\"Bug!\")\n",
        "\n",
        "  if tp == 0:\n",
        "    prec = 0\n",
        "    recall = 0\n",
        "  else:\n",
        "    prec = tp / (tp+fp)\n",
        "    recall = tp / (tp+fn)\n",
        "  if (prec == 0) and (recall == 0):\n",
        "    f1 = 0\n",
        "  else:\n",
        "    f1 = 2*prec*recall/(prec+recall)\n",
        "\n",
        "  return err, prec, recall, f1, tp, fp, tn, fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l45wX3uBKaRX"
      },
      "outputs": [],
      "source": [
        "# Convolutional filters are independent of the data (as their parameters are\n",
        "# sampled from random distributions), therefore the features based on\n",
        "# convolutional filters may be pre-calculated (so that they do not need to be\n",
        "# calculated in each round of the cross-validation which speeds up the\n",
        "# execution of the experiment)\n",
        "\n",
        "length = len(X[0][0])\n",
        "convolutional_filters = []\n",
        "if UNIVARIATE:\n",
        "  for i in range(10000):\n",
        "    convolutional_filters.append(get_random_convolution_params_univariate(i, length))\n",
        "\n",
        "  features_max, features_ppv, features_dtw_min = \\\n",
        "    get_rocket_features(X, convolutional_filters, apply_convolution_univariate)\n",
        "else:\n",
        "  for i in range(10000):\n",
        "    convolutional_filters.append(get_random_convolution_params(i, length))\n",
        "\n",
        "  features_max, features_ppv, features_dtw_min = \\\n",
        "    get_rocket_features(X, convolutional_filters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI9qXl_IcVca"
      },
      "outputs": [],
      "source": [
        "# 10-times 10-fold cross-validation\n",
        "\n",
        "all_err_rocket  = np.zeros( (8, 100) )\n",
        "all_err_ppv     = np.zeros( (8, 100) )\n",
        "all_err_max     = np.zeros( (8, 100) )\n",
        "all_err_erocket = np.zeros( (8, 100) )\n",
        "\n",
        "fold = 0\n",
        "\n",
        "for seed in range(10):\n",
        "  kf = StratifiedKFold(n_splits=10, random_state=seed+42, shuffle=True)\n",
        "  for train_index, test_index in kf.split(X, y):\n",
        "    print(f\"FOLD: {fold:2}\")\n",
        "    features_train_max = features_max[train_index]\n",
        "    features_train_ppv = features_ppv[train_index]\n",
        "    features_train_dtw_min = features_dtw_min[train_index]\n",
        "\n",
        "    features_test_max = features_max[test_index]\n",
        "    features_test_ppv = features_ppv[test_index]\n",
        "    features_test_dtw_min = features_dtw_min[test_index]\n",
        "\n",
        "    y_train = y[train_index]\n",
        "    y_test  = y[test_index]\n",
        "\n",
        "    cls = RidgeClassifier()\n",
        "    cls.fit( np.hstack((features_train_max, features_train_ppv)), y_train)\n",
        "    pred = cls.predict( np.hstack((features_test_max, features_test_ppv)) )\n",
        "    all_err_rocket[:,fold] = evaluate(pred, y_test)\n",
        "\n",
        "    cls = RidgeClassifier()\n",
        "    cls.fit( features_train_ppv, y_train )\n",
        "    pred = cls.predict( features_test_ppv )\n",
        "    all_err_ppv[:,fold] = evaluate(pred, y_test)\n",
        "\n",
        "    cls = RidgeClassifier()\n",
        "    cls.fit( features_train_max, y_train )\n",
        "    pred = cls.predict( features_test_max )\n",
        "    all_err_max[:,fold] = evaluate(pred, y_test)\n",
        "\n",
        "    cls = RidgeClassifier()\n",
        "    cls.fit( features_train_dtw_min, y_train )\n",
        "    pred = cls.predict( features_test_dtw_min )\n",
        "    all_err_erocket[:,fold] = evaluate(pred, y_test)\n",
        "\n",
        "    fold = fold + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9LuSRVDh1fc"
      },
      "outputs": [],
      "source": [
        "print( \"Method                    Classification error  p-values\")\n",
        "print(f\"ROCKET                    \"+\n",
        "      f\"{np.mean(all_err_rocket[0]):6.4f} +/- {np.std(all_err_rocket[0]):6.4f}  \")\n",
        "print(f\"ROCKET-PPV                \"+\n",
        "      f\"{np.mean(all_err_ppv[0]):6.4f} +/- {np.std(all_err_ppv[0]):6.4f}  \")\n",
        "print(f\"ROCKET-MAX                \"+\n",
        "      f\"{np.mean(all_err_max[0]):6.4f} +/- {np.std(all_err_max[0]):6.4f}  \")\n",
        "print(f\"EROCKET (our approach)    \"+\n",
        "      f\"{np.mean(all_err_erocket[0]):6.4f} +/- {np.std(all_err_erocket[0]):6.4f}     \"+\n",
        "      f\"{ttest_rel(all_err_rocket[0], all_err_erocket[0])[1]:5.3f}  \"+\n",
        "      f\"{ttest_rel(all_err_ppv[0], all_err_erocket[0])[1]:5.3f}  \"+\n",
        "      f\"{ttest_rel(all_err_max[0], all_err_erocket[0])[1]:5.3f}  \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn3qcmL3rGqq"
      },
      "outputs": [],
      "source": [
        "print(\"all_err_rocket=np.\"+repr(all_err_rocket))\n",
        "print(\"all_err_ppv=np.\"+repr(all_err_ppv))\n",
        "print(\"all_err_max=np.\"+repr(all_err_max))\n",
        "print(\"all_err_erocket=np.\"+repr(all_err_erocket))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGSs1nlNDWyO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPBCdIZQfMI0bm4mO8iYDh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}